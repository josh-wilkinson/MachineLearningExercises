{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"0.28.0"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11512852,"sourceType":"datasetVersion","datasetId":7219482}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning - Practical 1 - Linear Regression\n\nNames: Joshua Wilkinson  |      Ilia Leonov    |    Hoai Son Bien \n\nSummer Term 2024   ","metadata":{}},{"cell_type":"markdown","source":"This notebook provides you with the assignments and the overall code structure you need to complete the assignment. There are also questions that you need to answer in text form. Please use full sentences and reasonably correct spelling/grammar.\n\nRegarding submission & grading:\n\n- Work in groups of three and hand in your solution as a group.\n\n- Solutions need to be uploaded to StudIP until the submission date indicated in the course plan. Please upload a copy of this notebook and a PDF version of it after you ran it.\n\n- Solutions need to be presented to tutors in tutorial. Presentation dates are listed in the course plan. Every group member needs to be able to explain everything.\n\n- You have to solve N-1 practicals to get admission to the exam.\n\n- For plots you create yourself, all axes must be labeled. \n\n- Do not change the function interfaces.","metadata":{}},{"cell_type":"markdown","source":"## Imports\n\nJupyter Notebook provides the possibility of using libraries, functions and variables globally. This means, once you import the libraries, functions, etc. you won't have to import them again in the next cell. However, if for any reason you end the session (crash, timeout, etc.), then you'll have to run this cell to have your libraries imported again. So, let's go ahead and import whatever we need in this homework assignment.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2025-04-22T11:51:08.613854Z","iopub.execute_input":"2025-04-22T11:51:08.614164Z","iopub.status.idle":"2025-04-22T11:51:08.619653Z","shell.execute_reply.started":"2025-04-22T11:51:08.614104Z","shell.execute_reply":"2025-04-22T11:51:08.618879Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## The  dataset","metadata":{}},{"cell_type":"markdown","source":"The dataset consists of over 20.000 materials and lists their physical features. From these features, we want to learn how to predict the critical temperature, i.e. the temperature we need to cool the material to so it becomes superconductive. First load and familiarize yourself with the data set a bit.","metadata":{}},{"cell_type":"code","source":"# data = pd.read_csv('data/superconduct_train.csv')\ndata = pd.read_csv('/kaggle/input/superconduct-train-csv/Practicals/superconduct_train.csv')\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-22T11:53:09.833467Z","iopub.execute_input":"2025-04-22T11:53:09.833759Z","iopub.status.idle":"2025-04-22T11:53:10.862474Z","shell.execute_reply.started":"2025-04-22T11:53:09.833739Z","shell.execute_reply":"2025-04-22T11:53:10.861828Z"},"trusted":true},"outputs":[{"name":"stdout","text":"(21263, 82)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2025-04-22T11:53:13.330046Z","iopub.execute_input":"2025-04-22T11:53:13.330660Z","iopub.status.idle":"2025-04-22T11:53:13.365279Z","shell.execute_reply.started":"2025-04-22T11:53:13.330634Z","shell.execute_reply":"2025-04-22T11:53:13.364525Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n0                   4         88.944468             57.862692   \n1                   5         92.729214             58.518416   \n2                   4         88.944468             57.885242   \n3                   4         88.944468             57.873967   \n4                   4         88.944468             57.840143   \n\n   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n0          66.361592              36.116612             1.181795   \n1          73.132787              36.396602             1.449309   \n2          66.361592              36.122509             1.181795   \n3          66.361592              36.119560             1.181795   \n4          66.361592              36.110716             1.181795   \n\n   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n0                 1.062396          122.90607              31.794921   \n1                 1.057755          122.90607              36.161939   \n2                 0.975980          122.90607              35.741099   \n3                 1.022291          122.90607              33.768010   \n4                 1.129224          122.90607              27.848743   \n\n   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n0        51.968828  ...          2.257143       2.213364           2.219783   \n1        47.094633  ...          2.257143       1.888175           2.210679   \n2        51.968828  ...          2.271429       2.213364           2.232679   \n3        51.968828  ...          2.264286       2.213364           2.226222   \n4        51.968828  ...          2.242857       2.213364           2.206963   \n\n   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n0         1.368922             1.066221              1           1.085714   \n1         1.557113             1.047221              2           1.128571   \n2         1.368922             1.029175              1           1.114286   \n3         1.368922             1.048834              1           1.100000   \n4         1.368922             1.096052              1           1.057143   \n\n   std_Valence  wtd_std_Valence  critical_temp  \n0     0.433013         0.437059           29.0  \n1     0.632456         0.468606           26.0  \n2     0.433013         0.444697           19.0  \n3     0.433013         0.440952           22.0  \n4     0.433013         0.428809           23.0  \n\n[5 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>number_of_elements</th>\n      <th>mean_atomic_mass</th>\n      <th>wtd_mean_atomic_mass</th>\n      <th>gmean_atomic_mass</th>\n      <th>wtd_gmean_atomic_mass</th>\n      <th>entropy_atomic_mass</th>\n      <th>wtd_entropy_atomic_mass</th>\n      <th>range_atomic_mass</th>\n      <th>wtd_range_atomic_mass</th>\n      <th>std_atomic_mass</th>\n      <th>...</th>\n      <th>wtd_mean_Valence</th>\n      <th>gmean_Valence</th>\n      <th>wtd_gmean_Valence</th>\n      <th>entropy_Valence</th>\n      <th>wtd_entropy_Valence</th>\n      <th>range_Valence</th>\n      <th>wtd_range_Valence</th>\n      <th>std_Valence</th>\n      <th>wtd_std_Valence</th>\n      <th>critical_temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.862692</td>\n      <td>66.361592</td>\n      <td>36.116612</td>\n      <td>1.181795</td>\n      <td>1.062396</td>\n      <td>122.90607</td>\n      <td>31.794921</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>2.213364</td>\n      <td>2.219783</td>\n      <td>1.368922</td>\n      <td>1.066221</td>\n      <td>1</td>\n      <td>1.085714</td>\n      <td>0.433013</td>\n      <td>0.437059</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>92.729214</td>\n      <td>58.518416</td>\n      <td>73.132787</td>\n      <td>36.396602</td>\n      <td>1.449309</td>\n      <td>1.057755</td>\n      <td>122.90607</td>\n      <td>36.161939</td>\n      <td>47.094633</td>\n      <td>...</td>\n      <td>2.257143</td>\n      <td>1.888175</td>\n      <td>2.210679</td>\n      <td>1.557113</td>\n      <td>1.047221</td>\n      <td>2</td>\n      <td>1.128571</td>\n      <td>0.632456</td>\n      <td>0.468606</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.885242</td>\n      <td>66.361592</td>\n      <td>36.122509</td>\n      <td>1.181795</td>\n      <td>0.975980</td>\n      <td>122.90607</td>\n      <td>35.741099</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.271429</td>\n      <td>2.213364</td>\n      <td>2.232679</td>\n      <td>1.368922</td>\n      <td>1.029175</td>\n      <td>1</td>\n      <td>1.114286</td>\n      <td>0.433013</td>\n      <td>0.444697</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.873967</td>\n      <td>66.361592</td>\n      <td>36.119560</td>\n      <td>1.181795</td>\n      <td>1.022291</td>\n      <td>122.90607</td>\n      <td>33.768010</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.264286</td>\n      <td>2.213364</td>\n      <td>2.226222</td>\n      <td>1.368922</td>\n      <td>1.048834</td>\n      <td>1</td>\n      <td>1.100000</td>\n      <td>0.433013</td>\n      <td>0.440952</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>88.944468</td>\n      <td>57.840143</td>\n      <td>66.361592</td>\n      <td>36.110716</td>\n      <td>1.181795</td>\n      <td>1.129224</td>\n      <td>122.90607</td>\n      <td>27.848743</td>\n      <td>51.968828</td>\n      <td>...</td>\n      <td>2.242857</td>\n      <td>2.213364</td>\n      <td>2.206963</td>\n      <td>1.368922</td>\n      <td>1.096052</td>\n      <td>1</td>\n      <td>1.057143</td>\n      <td>0.433013</td>\n      <td>0.428809</td>\n      <td>23.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 82 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"Because the dataset is rather large, we prepare a small subset of the data as training set, and another subset as test set. To make the computations reproducible, we set the random seed. This makes the train and test splits same even if you re-run the notebook. Keeping the splits same is important for the fair models comparison.","metadata":{}},{"cell_type":"code","source":"target_clm = 'critical_temp'  # the critical temperature is our target variable\nn_trainset = 200  # size of the training set\nn_testset = 500  # size of the test set","metadata":{"execution":{"iopub.status.busy":"2025-04-22T11:53:27.411016Z","iopub.execute_input":"2025-04-22T11:53:27.411543Z","iopub.status.idle":"2025-04-22T11:53:27.414988Z","shell.execute_reply.started":"2025-04-22T11:53:27.411507Z","shell.execute_reply":"2025-04-22T11:53:27.414337Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# set random seed to make sure every test set is the same\nnp.random.seed(seed=1)\n\nidx = np.arange(data.shape[0])\nidx_shuffled = np.random.permutation(idx)  # shuffle indices to split into training and test set\n\ntest_idx = idx_shuffled[:n_testset]\ntrain_idx = idx_shuffled[n_testset:n_testset+n_trainset]\ntrain_full_idx = idx_shuffled[n_testset:]\n\nX_test = data.loc[test_idx, data.columns != target_clm].values\ny_test = data.loc[test_idx, data.columns == target_clm].values\nprint('Test set shapes (X and y)', X_test.shape, y_test.shape)\n\nX_train = data.loc[train_idx, data.columns != target_clm].values\ny_train = data.loc[train_idx, data.columns == target_clm].values\nprint('Small training set shapes (X and y):', X_train.shape, y_train.shape)\n\nX_train_full = data.loc[train_full_idx, data.columns != target_clm].values\ny_train_full = data.loc[train_full_idx, data.columns == target_clm].values\nprint('Full training set shapes (X and y):', X_train_full.shape, y_train_full.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-22T11:53:30.919942Z","iopub.execute_input":"2025-04-22T11:53:30.920252Z","iopub.status.idle":"2025-04-22T11:53:30.959229Z","shell.execute_reply.started":"2025-04-22T11:53:30.920228Z","shell.execute_reply":"2025-04-22T11:53:30.958488Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Test set shapes (X and y) (500, 81) (500, 1)\nSmall training set shapes (X and y): (200, 81) (200, 1)\nFull training set shapes (X and y): (20763, 81) (20763, 1)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Task 1: Plot the dataset\n\nTo explore the dataset, use `X_train_full` and `y_train_full` for two descriptive plots:\n\n* **Histogram** of the target variable. Use `plt.hist`.\n\n* **Scatter plots** relating the target variable to one of the feature values. For this you will need 81 scatter plots. Arrange them in one big figure with 9x9 subplots. Use `plt.scatter`. You may need to adjust the marker size and the alpha blending value. \n\nFurthermore, we need to normalize the data, such that each feature has a mean of zero mean and a variance of one. Implement a function `normalize` which normalizes the data. Print the means and standard variation of the first five features before and after.","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"# Histogram of the target variable\n","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:36:45.829Z","iopub.status.busy":"2020-05-28T07:36:45.809Z","iopub.status.idle":"2020-05-28T07:36:45.893Z","shell.execute_reply":"2020-05-28T07:36:46.455Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scatter plots of the target variable vs. features\n","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:36:45.936Z","iopub.status.busy":"2020-05-28T07:36:45.919Z","iopub.status.idle":"2020-05-28T07:37:00.826Z","shell.execute_reply":"2020-05-28T07:37:00.906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Which material properties may be useful for predicting superconductivity? What other observations can you make?","metadata":{}},{"cell_type":"markdown","source":" YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 2:  Implement your own OLS estimator\n\nWe want to use linear regression to predict the critical temperature. Implement the ordinary least squares estimator without regularization 'by hand':\n\n$w = (X^TX)^{-1}X^Ty$\n\nTo make life a bit easier, we provide a function that can be used to plot regression results. In addition it computes the mean squared error and the squared correlation between the true and predicted values. ","metadata":{}},{"cell_type":"code","source":"def plot_regression_results(y_test, y_pred, weights):\n    '''Produces three plots to analyze the results of linear regression:\n        -True vs predicted\n        -Raw residual histogram\n        -Weight histogram\n\n    Inputs:\n        y_test: (n_observations,) numpy array with true values\n        y_pred: (n_observations,) numpy array with predicted values\n        weights: (n_weights) numpy array with regression weights'''\n\n    print('MSE: ', mean_squared_error(y_test, y_pred))\n    print('r^2: ', r2_score(y_test, y_pred))\n\n    fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n    # predicted vs true\n    ax[0].scatter(y_test, y_pred, s=2)\n    ax[0].set_title('True vs. Predicted')\n    ax[0].set_xlabel('True %s' % (target_clm))\n    ax[0].set_ylabel('Predicted %s' % (target_clm))\n\n    # residuals\n    error = np.squeeze(np.array(y_test)) - np.squeeze(np.array(y_pred))\n    ax[1].hist(np.array(error), bins=30)\n    ax[1].set_title('Raw residuals')\n    ax[1].set_xlabel('(true-predicted)')\n\n    # weight histogram\n    ax[2].hist(weights, bins=30)\n    ax[2].set_title('weight histogram')\n\n    plt.tight_layout()","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:01.042Z","iopub.status.busy":"2020-05-28T07:37:00.996Z","iopub.status.idle":"2020-05-28T07:37:01.095Z","shell.execute_reply":"2020-05-28T07:37:01.193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As an example, we here show you how to use this function with random data. ","metadata":{}},{"cell_type":"code","source":"# weights is a vector of length 82: the first value is the intercept (beta0), then 81 coefficients\nweights = np.random.randn(82)\n\n# Model predictions on the test set\ny_pred_testing = np.random.randn(y_test.size) * np.max(y_test)\n\nplot_regression_results(y_test, y_pred_testing, weights)","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:01.171Z","iopub.status.busy":"2020-05-28T07:37:01.129Z","iopub.status.idle":"2020-05-28T07:37:02.128Z","shell.execute_reply":"2020-05-28T07:37:02.403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implement OLS linear regression yourself. Use `X_train` and `y_train` for estimating the weights and compute the MSE and $r^2$ from `X_test`. When you call our plotting function with the regression result, you should get mean squared error of 707.8.","metadata":{}},{"cell_type":"code","source":"def ols_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization on the training set and\n       returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:02.216Z","iopub.status.busy":"2020-05-28T07:37:02.181Z","iopub.status.idle":"2020-05-28T07:37:02.273Z","shell.execute_reply":"2020-05-28T07:37:02.424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plots of the results\n","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:02.350Z","iopub.status.busy":"2020-05-28T07:37:02.322Z","iopub.status.idle":"2020-05-28T07:37:02.989Z","shell.execute_reply":"2020-05-28T07:37:03.266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What do you observe? Is the linear regression model good?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 3: Compare your implementation to sklearn\n\nNow, familiarize yourself with the sklearn library. In the section on linear models:\n\nhttps://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n\nyou will find `sklearn.linear_model.LinearRegression`, the `sklearn` implementation of the OLS estimator. Use this sklearn class to implement OLS linear regression. Again obtain estimates of the weights on `X_train` and `y_train` and compute the MSE and $r^2$ on `X_test`.\n","metadata":{}},{"cell_type":"code","source":"def sklearn_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization using the sklearn library on the training set and\n       returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:03.074Z","iopub.status.busy":"2020-05-28T07:37:03.042Z","iopub.status.idle":"2020-05-28T07:37:03.124Z","shell.execute_reply":"2020-05-28T07:37:03.285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"weights, y_pred = sklearn_regression(X_test, X_train, y_train)\nplot_regression_results(y_test, y_pred, weights)","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:03.206Z","iopub.status.busy":"2020-05-28T07:37:03.180Z","iopub.status.idle":"2020-05-28T07:37:04.059Z","shell.execute_reply":"2020-05-28T07:37:04.195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you implemented everything correctly, the MSE is again 707.8.","metadata":{}},{"cell_type":"markdown","source":"Fit the model using the larger training set, `X_train_full` and `y_train_full`, and again evaluate on `X_test`.","metadata":{}},{"cell_type":"code","source":"weights, y_pred = sklearn_regression(X_test, X_train_full, y_train_full)\nplot_regression_results(y_test, y_pred, weights)","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:04.148Z","iopub.status.busy":"2020-05-28T07:37:04.114Z","iopub.status.idle":"2020-05-28T07:37:05.126Z","shell.execute_reply":"2020-05-28T07:37:05.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" How does test set performance change? What else changes?","metadata":{}},{"cell_type":"markdown","source":"YOU ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 4: Regularization with ridge regression\n\nWe will now explore how a penalty term on the weights can improve the prediction quality for finite data sets. Implement the analytical solution of ridge regression \n\n$w = (X^TX + \\alpha I_D)^{-1}X^Ty$\n\n\nas a function that can take different values of $\\alpha$, the regularization strength, as an input. In the lecture, this parameter was called $\\lambda$, but this is a reserved keyword in Python.","metadata":{}},{"cell_type":"code","source":"def ridge_regression(X_test, X_train, y_train, alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha\n       on the training set and returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n\n       Outputs:\n         weights: The weight vector for the regression model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:05.219Z","iopub.status.busy":"2020-05-28T07:37:05.181Z","iopub.status.idle":"2020-05-28T07:37:05.275Z","shell.execute_reply":"2020-05-28T07:37:05.574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the ridge regression on `X_train` with an alpha value of 10 and plot the obtained weights.","metadata":{}},{"cell_type":"code","source":"# Run ridge regression with alpha=10\n\n\n# Plot regression results\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now test a range of log-spaced $\\alpha\\text{s}$ (~10-20), which cover several orders of magnitude, e.g. from 10^-7 to 10^7. \n\n* For each $\\alpha$, you will get one model with one set of weights. \n* For each model, compute the error on the test set. \n\nStore both the errors and weights of all models for later use. You can use the function `mean_squared_error` from sklearn (imported above) to compute the MSE.\n","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 20)\n\n# ---------------- INSERT CODE ----------------------\n\n\n\n# ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:05.363Z","iopub.status.busy":"2020-05-28T07:37:05.331Z","iopub.status.idle":"2020-05-28T07:37:05.408Z","shell.execute_reply":"2020-05-28T07:37:05.587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Make a single plot that shows for each coefficient how it changes with $\\alpha$, i.e. one line per coefficient. Also think about which scale is appropriate for your $\\alpha$-axis. You can set this using `plt.xscale(...)`.","metadata":{}},{"cell_type":"code","source":"# Plot of coefficients vs. alphas\n","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:05.488Z","iopub.status.busy":"2020-05-28T07:37:05.458Z","iopub.status.idle":"2020-05-28T07:37:06.656Z","shell.execute_reply":"2020-05-28T07:37:06.776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Why are the values of the weights largest on the left? Do they all change monotonically? ","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"Plot how the performance (i.e. the error) changes as a function of $\\alpha$. As a sanity check, the MSE value for very small $\\alpha$ should be close to the test-set MSE of the unregularized solution, i.e. 708.","metadata":{}},{"cell_type":"code","source":"# Plot of MSE  vs. alphas\n","metadata":{"code_folding":[],"execution":{"iopub.execute_input":"2020-05-28T07:37:06.723Z","iopub.status.busy":"2020-05-28T07:37:06.694Z","iopub.status.idle":"2020-05-28T07:37:07.241Z","shell.execute_reply":"2020-05-28T07:37:07.508Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Which value of $\\alpha$ gives the minimum MSE? Is it better than the unregularized model? Why should the curve reach ~700 on the left?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"Now implement the same model using sklearn. Use the `linear_model.Ridge` object to do so.\n","metadata":{}},{"cell_type":"code","source":"def ridge_regression_sklearn(X_test, X_train, y_train, alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha using the sklearn\n       library on the training set and returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:07.315Z","iopub.status.busy":"2020-05-28T07:37:07.282Z","iopub.status.idle":"2020-05-28T07:37:07.361Z","shell.execute_reply":"2020-05-28T07:37:07.552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This time, only plot how the performance changes as a function of $\\alpha$. ","metadata":{}},{"cell_type":"code","source":"# Plot of MSE  vs. alphas\n","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:07.446Z","iopub.status.busy":"2020-05-28T07:37:07.410Z","iopub.status.idle":"2020-05-28T07:37:08.218Z","shell.execute_reply":"2020-05-28T07:37:08.534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note: Don't worry if the curve is not exactly identical to the one you got above. The loss function we wrote down in the lecture  has $\\alpha$ defined a bit differently compared to sklearn. However, qualitatively it should look the same.","metadata":{}},{"cell_type":"markdown","source":"## Task 5: Cross-validation\n\nUntil now, we always estimated the error on the test set directly. However, we typically do not want to tune hyperparameters of our inference algorithms like $\\alpha$ on the test set, as this may lead to overfitting. Therefore, we tune them on the training set using cross-validation. As discussed in the lecture, the training data is here split in `n_folds`-ways, where each of the folds serves as a held-out dataset in turn and the model is always trained on the remaining data. Implement a function that performs cross-validation for the ridge regression parameter $\\alpha$. You can reuse functions written above.","metadata":{}},{"cell_type":"code","source":"def ridgeCV(X, y, n_folds, alphas):\n    '''Runs a n_fold-crossvalidation over the ridge regression parameter alpha.\n       The function should train the linear regression model for each fold on all values of alpha.\n\n      Inputs:\n        X: (n_obs, n_features) numpy array - predictor\n        y: (n_obs,) numpy array - target\n        n_folds: integer - number of CV folds\n        alphas: (n_parameters,) - regularization strength parameters to CV over\n\n      Outputs:\n        cv_results_mse: (n_folds, len(alphas)) numpy array, MSE for each cross-validation fold\n\n      Note:\n        Fix the seed for reproducibility.\n    '''\n\n    cv_results_mse = np.zeros((n_folds, len(alphas)))\n    np.random.seed(seed=2)\n\n    # ---------------- INSERT CODE ----------------------\n\n\n\n    # ---------------- END CODE -------------------------\n\n    return cv_results_mse","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:08.317Z","iopub.status.busy":"2020-05-28T07:37:08.279Z","iopub.status.idle":"2020-05-28T07:37:08.375Z","shell.execute_reply":"2020-05-28T07:37:08.556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we run 10-fold cross-validation using the training data of a range of $\\alpha$s.","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 100)\nmse_cv = ridgeCV(X_train, y_train, n_folds=10, alphas=alphas)","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:08.472Z","iopub.status.busy":"2020-05-28T07:37:08.432Z","iopub.status.idle":"2020-05-28T07:37:09.886Z","shell.execute_reply":"2020-05-28T07:37:10.010Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We plot the MSE trace for each fold separately:","metadata":{}},{"cell_type":"code","source":"plt.plot(alphas, mse_cv.T, '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:09.963Z","iopub.status.busy":"2020-05-28T07:37:09.933Z","iopub.status.idle":"2020-05-28T07:37:10.445Z","shell.execute_reply":"2020-05-28T07:37:10.539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also plot the average across folds:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:10.500Z","iopub.status.busy":"2020-05-28T07:37:10.477Z","iopub.status.idle":"2020-05-28T07:37:10.934Z","shell.execute_reply":"2020-05-28T07:37:11.085Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What is the optimal $\\alpha$? Is it similar to the one found on the test set? Do the cross-validation MSE and the test-set MSE match well or differ strongly?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"We will now run cross-validation on the full training data. This will take a moment, depending on the speed of your computer. Afterwards, we will again plot the mean CV curves for the full data set (blue) and the small data set (orange).","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 100)\nmse_cv_full = ridgeCV(X_train_full, y_train_full, n_folds=10, alphas=alphas)","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:11.019Z","iopub.status.busy":"2020-05-28T07:37:10.978Z","iopub.status.idle":"2020-05-28T07:37:43.173Z","shell.execute_reply":"2020-05-28T07:37:43.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\nplt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:43.249Z","iopub.status.busy":"2020-05-28T07:37:43.218Z","iopub.status.idle":"2020-05-28T07:37:43.744Z","shell.execute_reply":"2020-05-28T07:37:43.860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We zoom in on the blue curve to the very left:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\nplt.xscale('log')\nminValue = np.min(np.mean(mse_cv_full, axis=0))\nplt.ylim([minValue-.01, minValue+.02])\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.execute_input":"2020-05-28T07:37:43.818Z","iopub.status.busy":"2020-05-28T07:37:43.794Z","iopub.status.idle":"2020-05-28T07:37:44.184Z","shell.execute_reply":"2020-05-28T07:37:44.214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Why does the CV curve on the full data set look so different? What is the optimal value of $\\alpha$ and why is it so much smaller than on the small training set?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}}]}